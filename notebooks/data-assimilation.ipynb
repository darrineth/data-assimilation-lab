{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "https://github.com/thiery-lab/data-assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model is a [three-dimensional dynamical system developed as a simpified model of atmosphereic convection](https://en.wikipedia.org/wiki/Lorenz_system) by Lorenz [1] which is a classic example of chaotic non-periodic dynamics. A flow map $\\varphi : \\mathbb{R}_{\\geq 0} \\to (\\mathbb{R}^3 \\to \\mathbb{R}^3)$ is defined such that $\\boldsymbol{x}(\\tau) = \\varphi(\\tau)(\\boldsymbol{x}_0)$ where $\\boldsymbol{x}(\\tau), \\tau > 0$ is the solution to the initial value problem\n",
    "\\begin{equation}\n",
    "  \\frac{\\mathrm{d} x_0}{\\mathrm{d} \\tau} = \\sigma (x_1 - x_0), \\quad\n",
    "  \\frac{\\mathrm{d} x_1}{\\mathrm{d} \\tau} = x_0 (\\rho - x_2) - x_1, \\quad\n",
    "  \\frac{\\mathrm{d} x_2}{\\mathrm{d} \\tau} = x_0 x_1 - \\beta x_2 \\quad\n",
    "  \\text{and} \\quad \\boldsymbol{x}(0) = \\boldsymbol{x}_0.\n",
    "\\end{equation}\n",
    "\n",
    "Here we use the (standard) parameter values $\\rho = 28, \\sigma=10, \\beta=\\frac{8}{3}$ which exhibit chaotic dynamics. An implicit mid-point method is used to define an approximate flow map $\\tilde{\\varphi}(\\Delta) \\approx \\varphi(\\Delta)$ with time step $\\Delta = 0.005$ and $S=10$ time steps per state update. The state update is $\\boldsymbol{x}_t = \\bigcirc_{s=1}^S \\big(\\tilde{\\varphi}({\\Delta})\\big)(\\boldsymbol{x}_{t-1}) + \\boldsymbol{\\xi}_t$ with additive noise with distribution $\\boldsymbol{\\xi}_t \\sim \\mathcal{N}(\\mathbf{0}_3, 0.1^2 \\mathbb{I}_3)$ and initial state distribution $\\boldsymbol{x}_0 \\sim \\mathcal{N}(\\mathbf{1}_3, 0.5^2 \\mathbb{I}_3)$. The first $x_{0,t}$ component of the state is noisily observed $y_t = x_{0,t} + \\epsilon_t$, with additive Gaussian noise $\\epsilon_t \\sim \\mathcal{N}(0, 5^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Lorenz, Edward Norton (1963). Deterministic nonperiodic flow. Journal of the Atmospheric Sciences. 20 (2): 130–141."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Assimilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dapy.filters as filters\n",
    "from dapy.models import Lorenz1963Model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'sigma': 10.,\n",
    "    'rho': 28.,\n",
    "    'beta': 8. / 3.,\n",
    "    'initial_state_mean': 1.,\n",
    "    'initial_state_std': 0.5,\n",
    "    'state_noise_std': 0.1,\n",
    "    'observation_function':  lambda x, t: x[..., 0:1],\n",
    "    'observation_noise_std': 5.,\n",
    "    'time_step': 0.005,\n",
    "    'num_integrator_step_per_update': 5,\n",
    "}\n",
    "model = Lorenz1963Model(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observation_time = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 0\n",
    "# Create an array of indices from 0..(num_observation_time-1)\n",
    "observation_time_indices = ??\n",
    "seed = 1234\n",
    "rng = np.random.default_rng(seed)\n",
    "state_sequence, observation_sequence = model.sample_state_and_observation_sequences(\n",
    "    rng, observation_time_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(observation_time_indices, state_sequence)\n",
    "ax.plot(observation_time_indices, observation_sequence, '.')\n",
    "ax.set_xlabel('Time index $t$')\n",
    "ax.set_ylabel('State')\n",
    "_ = ax.set_xlim(0, num_observation_time - 1)\n",
    "ax.legend(['$x_0(S\\Delta t)$', '$x_1(S\\Delta t)$', '$x_2(S\\Delta t)$', '$y_t$'], ncol=4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'projection': '3d'})\n",
    "ax.plot(*state_sequence.T, '-', lw=1)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_zlabel('$x_2$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer state from observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, observation_time_indices, state_sequence=None, \n",
    "                 plot_particles=False, plot_region=True, \n",
    "                 particle_skip=2, trace_alpha=0.5):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(12, 9))\n",
    "    for i, ax in zip(range(3), axes):\n",
    "        ax.plot(results['state_mean_sequence'][:, i], 'g-', lw=1, label='Est. mean')\n",
    "        if plot_region:\n",
    "            ax.fill_between(\n",
    "                observation_time_indices,\n",
    "                results['state_mean_sequence'][:, i] - 3 * results['state_std_sequence'][:, i],\n",
    "                results['state_mean_sequence'][:, i] + 3 * results['state_std_sequence'][:, i],\n",
    "                alpha=0.25, color='g', label='Est. mean ± 3 standard deviation'\n",
    "            )\n",
    "        if plot_particles:\n",
    "            lines = ax.plot(\n",
    "                observation_time_indices, results['state_particles_sequence'][:, ::particle_skip, i], \n",
    "                'r-', lw=0.25, alpha=trace_alpha)\n",
    "            lines[0].set_label('Particles')\n",
    "        if state_sequence is not None:\n",
    "            ax.plot(observation_time_indices, state_sequence[:, i], 'k--', label='Truth')\n",
    "        ax.set_ylabel('$x_{0}$'.format(i))\n",
    "        ax.legend(loc='upper center', ncol=4)\n",
    "    ax.set_xlabel('Time index $t$')\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/opt/static/Basic_concept_of_Kalman_filtering.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Transactions of the ASME -- Journal of Basic Engineering, Series D, 82, pp. 35--45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fkf = filters.FunctionKalmanFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fkf = fkf.filter(\n",
    "    model, observation_sequence, observation_time_indices, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_results(results_fkf, observation_time_indices, state_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Kalman filter (perturbed observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Kalman filter with perturbed observations.\n",
    "\n",
    "The filtering distribution at each observation time index is approximated by alternating propagating an ensemble of state particles forward through time under the model dynamics and linearly transforming the ensemble according to a Monte Carlo estimate of the Kalman filter assimilation update due to the observations at the current time index. Here a 'perturbed observation' ensemble Kalman filter assimilation update is used with an observation particle sampled for each state particle from the conditional distribution on the observation given the state, and these observation particles as well as the original state particles used to approximate the covariance and mean statistics used in the Kalman update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Evensen, G. (1994). Sequential data assimilation with nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics. Journal of Geophysical Research, 99 (C5), pp. 143--162\n",
    "\n",
    "Burgers, G.,van Leeuwen, P. J. and Evensen, G. (1998). Analysis scheme in the ensemble Kalman filter. Monthly Weather Review, (126) pp 1719--1724."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enkf = filters.EnsembleKalmanFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different values for num_particle\n",
    "# How does the result change?\n",
    "num_particle = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_enkf = enkf.filter(\n",
    "    model, observation_sequence, observation_time_indices, \n",
    "    num_particle=num_particle, rng=rng, return_particles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_results(results_enkf, observation_time_indices, state_sequence)\n",
    "axes[0].set_title('{} particles'.format(num_particle));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Transform Kalman filter (deterministic square root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Kalman filter with deterministic matrix square root updates.\n",
    "\n",
    "The filtering distribution at each observation time index is approximated by alternating propagating an ensemble of state particles forward through time under the model dynamics and linearly transforming the ensemble according to a Monte Carlo estimate of the Kalman filter assimilation update due to the observations at the current time index. Here a 'square-root' ensemble Kalman filter assimilation update is used, which requires that the model has Gaussian observation noise with a known covariance, but compared to the 'perturbed observation' variant avoids the additional variance associated with sampling pseudo-observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Bishop, C. H. Etherton, B. J. and  Majumdar, S. J. (2001). Adaptive sampling with the ensemble transform Kalman filter. Part I: Theoretical aspects.Mon. Wea. Rev., 129, 420–436.\n",
    "2. M. K. Tippett, J. L. Anderson, C. H. Bishop, T. M. Hamill, and J. S. Whitaker, Ensemble square root filters, Monthly Weather Review, 131 (2003), pp. 1485--1490."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etkf = filters.EnsembleTransformKalmanFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different values for num_particle\n",
    "# How does the result change?\n",
    "num_particle = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_etkf = etkf.filter(\n",
    "    model, observation_sequence, observation_time_indices, \n",
    "    num_particle=num_particle, rng=rng, return_particles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plot_results(results_etkf, observation_time_indices, state_sequence)\n",
    "axes[0].set_title('{} particles'.format(num_particle));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3\n",
    "\n",
    "Try different parameters for the model. Does it still exhibit chaotic properties? How do different filters adapt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservoir computing\n",
    "\n",
    "Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir. After the input signal is fed into the reservoir, which is treated as a \"black box,\" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed. The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be utilized to reduce the effective computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Tanaka, Gouhei; Yamane, Toshiyuki; Héroux, Jean Benoit; Nakane, Ryosho; Kanazawa, Naoki; Takeda, Seiji; Numata, Hidetoshi; Nakano, Daiju; Hirose, Akira (2019). \"Recent advances in physical reservoir computing: A review\". Neural Networks. 115: 100–123. doi:10.1016/j.neunet.2019.03.005. ISSN 0893-6080. PMID 30981085.\n",
    "2. Röhm, André; Lüdge, Kathy (2018-08-03). \"Multiplexed networks: reservoir computing with virtual and real nodes\". Journal of Physics Communications. 2 (8): 085007. Bibcode:2018JPhCo...2h5007R. doi:10.1088/2399-6528/aad56d. ISSN 2399-6528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/opt/static/reservior_computing.jpeg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Echo state network\n",
    "\n",
    "The echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mackey-Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/tmp/pyESN')\n",
    "from pyESN import ESN\n",
    "data = np.load('/tmp/pyESN/mackey_glass_t17.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set the number of output layers\n",
    "# Experiment with different values for n_reservoir\n",
    "# How does the result change?\n",
    "esn = ESN(n_inputs = 1,\n",
    "          n_outputs = ??,\n",
    "          n_reservoir = ??,\n",
    "          spectral_radius = 1.5,\n",
    "          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlen = 1000\n",
    "future = 1000\n",
    "pred_training = esn.fit(np.ones(trainlen),data[:trainlen])\n",
    "prediction = esn.predict(np.ones(future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test error: \\n\"+str(np.sqrt(np.mean((prediction.flatten() - data[trainlen:trainlen+future])**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,1.5))\n",
    "plt.plot(range(0,trainlen+future),data[0:trainlen+future],'k',label=\"target system\")\n",
    "plt.plot(range(trainlen,trainlen+future),prediction,'r', label=\"free running ESN\")\n",
    "lo,hi = plt.ylim()\n",
    "plt.plot([trainlen,trainlen],[lo+np.spacing(1),hi-np.spacing(1)],'k:')\n",
    "plt.legend(loc=(0.61,1.1),fontsize='x-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lorenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observation_time = 4000\n",
    "observation_time_indices = np.arange(num_observation_time)\n",
    "seed = 1234\n",
    "rng = np.random.default_rng(seed)\n",
    "data, observation_sequence = model.sample_state_and_observation_sequences(\n",
    "    rng, observation_time_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set the number of output layers\n",
    "# Experiment with different values for n_reservoir\n",
    "# How does the result change?\n",
    "esn = ESN(n_inputs = 1,\n",
    "          n_outputs = ??,\n",
    "          n_reservoir = ??,\n",
    "          spectral_radius = 1.5,\n",
    "          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlen = 2000\n",
    "future = 2000\n",
    "pred_training = esn.fit(np.ones(trainlen),data[:trainlen])\n",
    "prediction = esn.predict(np.ones(future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "#TODO: Show prediction result on plot\n",
    "ax.plot(??,??,??, lw=0.5)\n",
    "ax.plot(data[future:,0], data[future:,1], data[future:,2], lw=0.5)\n",
    "ax.set_xlabel(\"X Axis\")\n",
    "ax.set_ylabel(\"Y Axis\")\n",
    "ax.set_zlabel(\"Z Axis\")\n",
    "ax.set_title(\"Lorenz Attractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin price forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('/opt/static/BTC-USD.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set the number of output layers\n",
    "# Experiment with different values for n_reservoir\n",
    "# How does the result change?\n",
    "esn = ESN(n_inputs = 1,\n",
    "          n_outputs = 1,\n",
    "          n_reservoir = 400,\n",
    "          spectral_radius = 1.5,\n",
    "          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different values for trainlen & feature (sum have to be equal 367)\n",
    "# How does the result change?\n",
    "trainlen = ??\n",
    "future = ??\n",
    "pred_training = esn.fit(np.ones(trainlen),data[:trainlen])\n",
    "prediction = esn.predict(np.ones(future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,trainlen+future),data[0:trainlen+future],'k',label=\"target system\")\n",
    "plt.plot(range(trainlen,trainlen+future),prediction,'r', label=\"free running ESN\")\n",
    "lo,hi = plt.ylim()\n",
    "plt.plot([trainlen,trainlen],[lo+np.spacing(1),hi-np.spacing(1)],'k:')\n",
    "plt.legend(loc=(0.61,1.1),fontsize='x-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
